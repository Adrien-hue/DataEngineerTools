{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Votre premier projet\n",
    "\n",
    "## ATTENTION : Toutes les commandes de ce Notebook doivent être lancées directement depuis un terminal \n",
    "\n",
    "Dans un premier temps vous devez créer un projet Scrapy avec la commande\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy startproject newscrawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette commande va créer un dossier `monprojet` contenant les éléments\n",
    "suivants correspondant au squelette :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newscrawler/\n",
    "    scrapy.cfg            # Options de déploiement\n",
    "\n",
    "    newscrawler/             # Le module Python contenant les informations\n",
    "        __init__.py\n",
    "\n",
    "        items.py          # Fichier contenant les items\n",
    "\n",
    "        middlewares.py    # Fichier contenant les middlewares\n",
    "\n",
    "        pipelines.py      # Fichier contenant les pipelines\n",
    "\n",
    "        settings.py       # Fichier contenant les paramètres du projet\n",
    "\n",
    "        spiders/          # Dossier contenant toutes les spiders\n",
    "            __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Votre première Spider\n",
    "\n",
    "Une Spider est une classe Scrapy qui permet de mettre en place toute\n",
    "l'architecture complexe vue dans l'introduction. Pour définir une\n",
    "spider, il vous faut hériter de la classe $scrapy.Spider$. La seule\n",
    "chose à faire est de définir la première requête à effectuer et comment\n",
    "suivre les liens. La Spider s'arrêtera lorsqu'elle aura parcouru tous\n",
    "les liens qu'on lui a demandé de suivre.\n",
    "\n",
    "Pour créer une Spider on utilise la syntaxe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy genspider <SPIDER_NAME> <DOMAIN_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par exemple,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd newscrawler\n",
    "scrapy genspider lemonde lemonde.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette commande permet de créer une spider appelée `lemonde` pour scraper\n",
    "le domaine `lemonde.fr`. Cela crée le fichier Python\n",
    "`spiders/lemonde.py` suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class LemondeSpider(scrapy.Spider):\n",
    "    name = \"lemonde\"\n",
    "    allowed_domains = [\"www.lemonde.fr\"]\n",
    "    start_urls = ['http://www.lemonde.fr/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une bonne pratique pour commencer à développer une Spider est de passer\n",
    "par l'interface Shell proposée par Scrapy. Elle permet de récupérer un\n",
    "objet `Response` et de tester les méthodes de récupération des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy shell 'http://lemonde.fr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les utilisateurs de windows il vous faut mettre des doubles quotes\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy shell \"http://lemonde.fr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy lance un kernel Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2018-12-02 16:05:50 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: newscrawler)\n",
    "2018-12-02 16:05:50 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'newscrawler', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'NEWSPIDER_MODULE': 'newscrawler.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['newscrawler.spiders']}\n",
    "2018-12-02 16:05:50 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    "'scrapy.extensions.telnet.TelnetConsole']\n",
    "2018-12-02 16:05:50 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    "'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    "'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    "'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    "'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    "'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    "'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    "'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    "'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    "'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    "'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2018-12-02 16:05:50 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    "'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    "'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    "'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    "'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2018-12-02 16:05:50 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2018-12-02 16:05:50 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
    "2018-12-02 16:05:50 [scrapy.core.engine] INFO: Spider opened\n",
    "2018-12-02 16:05:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.lemonde.fr/robots.txt> (referer: None)\n",
    "2018-12-02 16:05:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.lemonde.fr/> (referer: None)\n",
    "2018-12-02 16:05:54 [traitlets] DEBUG: Using default logger\n",
    "2018-12-02 16:05:54 [traitlets] DEBUG: Using default logger\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x10fc38c18>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET https://www.lemonde.fr/>\n",
    "[s]   response   <200 https://www.lemonde.fr/>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x113bb0898>\n",
    "[s]   spider     <DefaultSpider 'default' at 0x113e60cc0>\n",
    "[s] Useful shortcuts:\n",
    "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
    "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects\n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   view(response)    View response in a browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce à cette interface, vous avez accès à plusieurs objets comme la\n",
    "`Response`, la `Request`, la `Spider` par exemple. Vous pouvez aussi\n",
    "exécuter `view(response)` pour afficher ce que Scrapy récupère dans un\n",
    "navigateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]: response\n",
    "Out[1]: <200 https://www.lemonde.fr/>\n",
    "\n",
    "In [3]: request\n",
    "Out[3]: <GET https://www.lemonde.fr/>\n",
    "\n",
    "In [4]: type(request)\n",
    "Out[4]: scrapy.http.request.Request\n",
    "\n",
    "In [5]: spider\n",
    "Out[5]: <LemondeSpider 'lemonde' at 0x1080fccc0>\n",
    "\n",
    "In [6]: type(spider)\n",
    "Out[6]: monprojet.spiders.lemonde.LeMondeSpider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on voit que la Spider est une instance de LemondeSpider. Lorsqu'on\n",
    "lance le $scrapy shell$ scrapy va chercher dans les spiders si une\n",
    "correspond au lien passé en paramètre, si oui , il l'utilise sinon une\n",
    "$DefaultSpider$ est instanciée.\n",
    "\n",
    "## Vos premières requêtes\n",
    "\n",
    "On peut commencer à regarder comment extraire les données de la page web\n",
    "en utilisant le langage de requêtes proposé par Scrapy. Il existe deux\n",
    "types de requêtes : les requêtes `css` et `xpath`. Les requêtes `xpath`\n",
    "sont plus complexes mais plus puissantes que les requêtes `css`. Dans le\n",
    "cadre de ce tutorial, nous allons uniquement aborder les requêtes `css`,\n",
    "elles nous suffiront pour extraire les données dont nous avons besoin\n",
    "(en interne, Scrapy transforme les requêtes `css`en requêtes `xpath`.\n",
    "\n",
    "Que ce soit les requêtes `css` ou `xpath`, elles crééent des sélecteurs\n",
    "de différents types. Quelques exemples :\n",
    "\n",
    "Pour récupérer le titre d'une page :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]: response.css('title')\n",
    "Out[1]: [<Selector xpath='descendant-or-self::title' data='<title>Le Monde.fr - Actualités et Infos'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère une liste de sélecteurs correspondant à la requête `css`\n",
    "appelée. La requête précédente était unique, d'autres requêtes moins\n",
    "restrictives permettent de récupérer plusieurs résultats. Par exemple\n",
    "pour rechercher l'ensemble des liens présents sur la page, on va\n",
    "rechercher les balises HTML `<a></a>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [5]: response.css(\"a\")[0:10]\n",
    "Out[5]:\n",
    "[<Selector xpath='descendant-or-self::a' data='<a target=\"_blank\" data-target=\"jelec-he'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/\"> <div class=\"logo__lemonde l'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"https://secure.lemonde.fr/sfuse'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"https://abo.lemonde.fr/#xtor=CS'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/\" class=\"Burger__right-arrow j'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/\" class=\"Burger__right-arrow j'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"#\" class=\"js-dropdown Burger__r'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/mouvement-des-gilets-jaunes/\" '>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/carlos-ghosn/\" data-suggestion'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/implant-files/\" data-suggestio'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer le texte contenu dans les balises, on passe le paramètre\n",
    "`<TAG>::text`. Par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [6]: response.css(\"title::text\")\n",
    "Out[6]: [<Selector xpath='descendant-or-self::title/text()' data='Le Monde.fr - Actualités et Infos en Fra'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "\n",
    "Exercice\n",
    "\n",
    "  Comparer les résultats des deux requêtes `response.css('title')` et\n",
    "`response.css('title::text')`.\n",
    "\n",
    "Maintenant pour extraire les données des selecteurs on utilise l'une des\n",
    "deux méthodes suivantes : - `extract()` permet de récupérer une liste\n",
    "des données extraites de tous les sélecteurs - `extract_first()` permet\n",
    "de récupérer une `String` provenant du premier sélecteur de la liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [7]: response.css('title::text').extract_first()\n",
    "Out[7]: 'Le Monde.fr - Actualités et Infos en France et dans le monde'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut récupérer un attribut d'une balise avec la syntaxe\n",
    "`<TAG>::attr(<ATTRIBUTE_NAME>)` :\n",
    "\n",
    "Par exemple, les liens sont contenus dans un attribut `href`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [9]: response.css('a::attr(href)')[0:10]\n",
    "Out[9]:\n",
    "[<Selector xpath='descendant-or-self::a/@href' data='https://journal.lemonde.fr'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='https://secure.lemonde.fr/sfuser/connexi'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='https://abo.lemonde.fr/#xtor=CS1-454[CTA'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='#'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/mouvement-des-gilets-jaunes/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/carlos-ghosn/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/implant-files/'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vu précédemment, si on veut récupérer la liste des liens de la page on applique la méthode $extract()$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [11]: response.css('a::attr(href)').extract()[0:10]\n",
    "Out[11]:\n",
    "['https://journal.lemonde.fr',\n",
    "'/',\n",
    "'https://secure.lemonde.fr/sfuser/connexion',\n",
    "'https://abo.lemonde.fr/#xtor=CS1-454[CTA_LMFR]-[HEADER]-5-[Home]',\n",
    "'/',\n",
    "'/',\n",
    "'#',\n",
    "'/mouvement-des-gilets-jaunes/',\n",
    "'/carlos-ghosn/',\n",
    "'/implant-files/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les liens dans une page HTML sont souvent codés de manière relative par\n",
    "rapport à la page courante. La méthode de l'objet `Response` peut être\n",
    "utilisée pour recréer l'url complet.\n",
    "\n",
    "Un exemple sur le 4e élément :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [14]: response.urljoin(response.css('a::attr(href)').extract()[8])\n",
    "Out[14]: 'https://www.lemonde.fr/carlos-ghosn/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alors que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [15]: response.css('a::attr(href)').extract()[8]\n",
    "Out[15]: '/carlos-ghosn/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "\n",
    "Exercice : Utiliser une liste compréhension pour transformer les 10\n",
    "premiers liens relatifs récupérés par la méthode `extract()` en liens\n",
    "absolus.    \n",
    "\n",
    "Le résultat doit ressembler à :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out[23]: \n",
    "['https://journal.lemonde.fr',\n",
    "'https://www.lemonde.fr/',\n",
    "'https://secure.lemonde.fr/sfuser/connexion',\n",
    "'https://abo.lemonde.fr/#xtor=CS1-454[CTA_LMFR]-[HEADER]-5-[Home]',\n",
    "'https://www.lemonde.fr/',\n",
    "'https://www.lemonde.fr/',\n",
    "'https://www.lemonde.fr/',\n",
    "'https://www.lemonde.fr/mouvement-des-gilets-jaunes/',\n",
    "'https://www.lemonde.fr/carlos-ghosn/',\n",
    "'https://www.lemonde.fr/implant-files/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Des requêtes plus complexes\n",
    "\n",
    "On peut créer des requêtes plus complexes en utilisant à la fois la\n",
    "structuration HTML du document mais également la couche de présentation\n",
    "CSS. On utilise l'inspecteur de `Google Chrome` pour identifier le type\n",
    "et l'identifiant de la balise contenant les informations.\n",
    "\n",
    "Il y a au moins deux choses à savoir en `css` :  \n",
    "\n",
    "-   Les `.` représentent les classes\n",
    "-   Les `#` représentent les id\n",
    "\n",
    "On se propose de récupérer toutes les sous-catégories de news dans la\n",
    "catégorie **Actualités**. On remarque en utilisant l'inspecteur\n",
    "d'élement de Chrome que toutes les catégories sont rangées dans une\n",
    "balise avec l'id $#nav-markup$ ensuite dans les classes $Nav__item$.\n",
    "\n",
    "A partir de cette structure HTML on peut construire la requête suivante\n",
    "pour récupérer la barre de navigation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [19]: response.css(\"#nav-markup\")\n",
    "Out[19]: [<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']\" data='<ul id=\"nav-markup\"> <li class=\"Nav__ite'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite pour récupérer les différentes catégories :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [24]: response.css(\"#nav-markup .Nav__item\")\n",
    "Out[24]:\n",
    "[<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item js-burger-to-show N'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item Nav__item--home Nav'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"/\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"/recherc'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut maintenant retourner tous les liens présents dans cette\n",
    "catégorie. On remarque qu'elle apparait à la 4eme position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [34]: response.css(\"#nav-markup .Nav__item\")[3]\n",
    "Out[34]: <Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant pour récupérer tous les liens on peut chainer les requêtes.\n",
    "On accède alors à toutes les balises $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [35]: response.css(\"#nav-markup .Nav__item\")[3].css(\"a\")\n",
    "Out[35]:\n",
    "[<Selector xpath='descendant-or-self::a' data='<a href=\"#\" class=\"js-dropdown Burger__r'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/mouvement-des-gilets-jaunes/\" '>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/carlos-ghosn/\" data-suggestion'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/implant-files/\" data-suggestio'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/climat/\" data-suggestion>Clima'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/affaire-khashoggi/\" data-sugge'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/emmanuel-macron/\" data-suggest'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/ukraine/\" data-suggestion>Ukra'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/russie/\" data-suggestion>Russi'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/referendum-sur-le-brexit/\" dat'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/harcelement-sexuel/\" data-sugg'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/actualite-en-continu/\" data-su'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/international/\">International<'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/politique/\">Politique</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/societe/\">Société</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/les-decodeurs/\">Les Décodeurs<'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/sport/\">Sport</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/planete/\">Planète</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/sciences/\">Sciences</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/campus/\">M Campus</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/afrique/\">Le Monde Afrique</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/pixels/\">Pixels</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/actualite-medias/\">Médias</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/sante/\">Santé</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/big-browser/\">Big Browser</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/disparitions/\">Disparitions</a'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et pour récupérer les titres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [37]: response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract()\n",
    "Out[37]:\n",
    "['Actualités',\n",
    "'Mouvement des \"gilets jaunes\"',\n",
    "'Carlos Ghosn',\n",
    "'Implant Files',\n",
    "'Climat',\n",
    "'Affaire Khashoggi',\n",
    "'Emmanuel Macron',\n",
    "'Ukraine',\n",
    "'Russie',\n",
    "'Brexit',\n",
    "'Harcèlement sexuel',\n",
    "'Toute l’actualité en continu',\n",
    "'International',\n",
    "'Politique',\n",
    "'Société',\n",
    "'Les Décodeurs',\n",
    "'Sport',\n",
    "'Planète',\n",
    "'Sciences',\n",
    "'M Campus',\n",
    "'Le Monde Afrique',\n",
    "'Pixels',\n",
    "'Médias',\n",
    "'Santé',\n",
    "'Big Browser',\n",
    "'Disparitions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le shell Scrapy permet de définir la structure des requêtes et de\n",
    "s'assurer de la pertinence du résultat retourné. Pour automatiser le\n",
    "processus, il faut intégrer cette syntaxe au code Python des modules de\n",
    "spider définis dans la structure du projet.\n",
    "\n",
    "## Intégration des requêtes\n",
    "\n",
    "Le squelette de la classe `LeMondeSpider` généré lors de la création du\n",
    "projet doit maintenant être enrichi. Par défaut 3 attributs et une\n",
    "méthode `parse()` ont été créés :\n",
    "\n",
    "-   `name` permet d'identifier sans ambiguïté la spider dans le code.\n",
    "-   `allowed_domain` permet de filtrer les requêtes et forcer la spider\n",
    "    à rester sur une liste de domaines.\n",
    "-   `starts_urls` est la liste des urls d'où la spider va partir pour\n",
    "    commencer son scraping.\n",
    "-   `parse()` est une méthode héritée de la classe `scrapy.Spider`. Elle\n",
    "    doit être redéfinie selon les requêtes que l'on doit effectuer et\n",
    "    sera appelée sur l'ensemble des urls contenus dans la liste\n",
    "    `starts_urls`.\n",
    "\n",
    "`parse()` est une fonction `callback` qui sera appelée automatiquement\n",
    "sur chaque objet `Response` retourné par la requête. Cette fonction est\n",
    "appelée de manière asynchrone. Plusieurs requêtes peuvent ainsi être\n",
    "lancées en parallèles sans bloquer le thread principal. L'objet\n",
    "`Response` passé en paramètre est le même que celui mis à disposition\n",
    "lors de l'exécution du Scrapy Shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    title = response.css('title::text').extract_first()\n",
    "    all_links = {\n",
    "        name:response.urljoin(url) for name, url in zip(\n",
    "        response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract(),\n",
    "        response.css(\"#nav-markup .Nav__item\")[3].css(\"a::attr(href)\").extract())\n",
    "    }\n",
    "    yield {\n",
    "        \"title\":title,\n",
    "        \"all_links\":all_links\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction est un générateur (`yield`) et retourne un dictionnaire\n",
    "composé de deux éléments :\n",
    "\n",
    "-   Le titre de la page;\n",
    "-   La liste des liens sortants sous forme de String.\n",
    "\n",
    "Pour le moment cette spider ne parcourt que la page d'accueil, ce qui\n",
    "n'est pas très productif.\n",
    "\n",
    "## Votre premier scraper\n",
    "\n",
    "Récupérer les données sur un ensemble de pages webs nécessite d'explorer\n",
    "en profondeur la structure du site en suivant tout ou partie des liens\n",
    "rencontrés.\n",
    "\n",
    "La spider peut se `balader` sur un site assez efficacement. Il suffit de\n",
    "lui indiquer comment faire. Il faut spécifier à Scrapy de générer une\n",
    "requête vers une nouvelle page en construisant l'objet `Request`\n",
    "correspondant. Ce nouvel objet `Request` est alors inséré dans le\n",
    "scheduler de Scrapy. On peut évidemment générer plusieurs `Request`\n",
    "simultanément, correspondant par exemple, à différents liens sur la page\n",
    "courante. Ils sont insérés séquentiellement dans le scheduler.\n",
    "\n",
    "Pour cela on modifie la méthode `parse()` de façon à ce qu'elle retourne\n",
    "un objet `Request` pour chaque nouveau lien rencontré. On associe\n",
    "également à cet objet une fonction de callback qui déterminera la\n",
    "manière dont cette nouvelle page doit être extraite.\n",
    "\n",
    "Par exemple, pour que la spider continue dans les liens des différentes\n",
    "régions (pour l'instant la fonction de callback ne fait rien) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy import Request\n",
    "\n",
    "\n",
    "\n",
    "class LemondeSpider(scrapy.Spider):\n",
    "    name = \"lemonde\"\n",
    "    allowed_domains = [\"www.lemonde.fr\"]\n",
    "    start_urls = ['https://www.lemonde.fr']\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').extract_first()\n",
    "        all_links = {\n",
    "            name:response.urljoin(url) for name, url in zip(\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract(),\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::attr(href)\").extract())\n",
    "        }\n",
    "        yield {\n",
    "            \"title\":title,\n",
    "            \"all_links\":all_links\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut ensuite *entrer* dans les liens des différentes sous-catégories\n",
    "pour récupérer les articles. Pour cela, nous créons une méthode\n",
    "`parse_category()` prend en argument un objet `Response` qui sera la\n",
    "réponse correspondant aux liens des régions. On peut comme ceci\n",
    "traverser un site en définissant des méthodes différentes en fonction du\n",
    "type de contenu.\n",
    "\n",
    "Si la structure du site est plus profonde, on peut empiler autant de\n",
    "couches que souhaité.\n",
    "\n",
    "Quand on arrive sur une page d'une sous-catégorie, on peut vouloir\n",
    "récupérer tous les éléments de la page. Pour cela, on réutilise le\n",
    "scrapy Shell pour commencer le développement de la nouvelle méthode\n",
    "d'extraction.\n",
    "\n",
    "Par exemple pour la page `https://www.lemonde.fr/international/` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy shell 'https://www.lemonde.fr/international/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fil des articles est stocké dans une balise avec la classe\n",
    "$class=fleuve$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [3]: response.css(\".fleuve\")\n",
    "Out[3]:\n",
    "[<Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' fleuve ')]\" data='<div class=\"fleuve\">\\n   <section>\\n      '>,\n",
    "<Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' fleuve ')]\" data='<div class=\"fleuve\">\\n</div>'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer chacun des articles, il faut adresser les balises\n",
    "`<article>` contenues dans le sélecteur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [4]: response.css(\".fleuve\")[0].css(\"article\")\n",
    "Out[4]:\n",
    "[<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi mg'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précédemment, on peut empiler les sélecteurs `css` pour créer des\n",
    "requêtes plus complexes.\n",
    "\n",
    "Par exemple, pour récupérer tous les titres des différents articles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [8]: response.css(\".fleuve\")[0].css(\"article h3 a::text\").extract()\n",
    "Out[8]:\n",
    "['Des dizaines de milliers de Géorgiens contestent dans la rue l’élection de Salomé Zourabichvili\\r\\n\\r\\n\\r\\n',\n",
    "'A Budapest en Hongrie, un îlot décroissant pour favoriser la transition\\r\\n\\r\\n\\r\\n',\n",
    "'En Israël, la police recommande l’inculpation de Nétanyahou dans une troisième enquête\\r\\n\\r\\n\\r\\n',\n",
    "'Donald Trump veut «\\xa0mettre fin\\xa0» à l’Aléna rapidement\\r\\n\\r\\n\\r\\n',\n",
    "'Le cauchemar de la «\\xa0rééducation\\xa0» des musulmans en Chine\\r\\n\\r\\n',\n",
    "'\\r\\n',\n",
    "'«\\xa0AMLO\\xa0» lance sa transformation du Mexique\\r\\n\\r\\n\\r\\n',\n",
    "'«\\xa0Paris brûle\\xa0»\\xa0: les médias étrangers relatent le «\\xa0chaos\\xa0» en marge des défilés des «\\xa0gilets jaunes\\xa0»\\r\\n\\r\\n\\r\\n',\n",
    "'Andrés Manuel Lopez Obrador intronisé président du Mexique\\r\\n\\r\\n\\r\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En HTML les données sont souvent de très mauvaise qualité. Il faut\n",
    "définir des méthodes permettant de les nettoyer pour être intégrées dans\n",
    "des bases de données.\n",
    "\n",
    "Par exemple, pour supprimer tous les espaces superflus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [13]: def clean_spaces(string_):\n",
    "...:        if string_ is not None: \n",
    "...:            return \" \".join(string_.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'appliquer à tous les titres récupérés, on peut faire une list\n",
    "comprehension : .. code-block:: Python\n",
    "\n",
    "> In \\[11\\]: \\[clean\\_spaces(article) for article in\n",
    "> response.css(\".fleuve\")\\[0\\].css(\"article h3 a::text\").extract()\\]\n",
    "> Out\\[11\\]: \\['Des dizaines de milliers de Géorgiens contestent dans la\n",
    "> rue l’élection de Salomé Zourabichvili', 'A Budapest en Hongrie, un\n",
    "> îlot décroissant pour favoriser la transition', 'En Israël, la police\n",
    "> recommande l’inculpation de Nétanyahou dans une troisième enquête',\n",
    "> 'Donald Trump veut « mettre fin » à l’Aléna rapidement', 'Le cauchemar\n",
    "> de la « rééducation » des musulmans en Chine', '', '« AMLO » lance sa\n",
    "> transformation du Mexique', '« Paris brûle » : les médias étrangers\n",
    "> relatent le « chaos » en marge des défilés des « gilets jaunes »',\n",
    "> 'Andrés Manuel Lopez Obrador intronisé président du Mexique'\\]\n",
    "\n",
    "La méthode précédente est intéressante si l'on ne recherche qu'une seule\n",
    "information par article.\n",
    "\n",
    "Par contre si l'on veut récupérer d'autres caractéristiques comme\n",
    "l'image ou la description par exemple, il est plus intéressant et plus\n",
    "efficace de récupérer l'objet et d'effectuer plusieurs traitements sur\n",
    "ce dernier.\n",
    "\n",
    "Chaque objet retourné par les requêtes `css` est un selecteur avec\n",
    "lequel on peut interagir.\n",
    "\n",
    "Par exemple pour récupérer le titre et le prix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [25]: for article in response.css(\".fleuve\")[0].css(\"article\"):\n",
    "...:     title = clean_spaces(article.css(\"h3 a::text\").extract_first())\n",
    "...:     image = article.css(\"img::attr(data-src)\").extract_first()\n",
    "...:     description = article.css(\".txt3::text\").extract_first()\n",
    "...:     print(f\"Title {title} \\nImage {image}\\nDescription {description}\\n ----\")\n",
    "\n",
    "Title Des dizaines de milliers de Géorgiens contestent dans la rue l’élection de Salomé Zourabichvili\n",
    "Image https://s1.lemde.fr/image/2018/12/02/147x97/5391641_7_5874_les-partisans-de-l-opposant-grigol-vashadze_20d2e8693a49b83fd3c5578f7799ae9c.jpg\n",
    "Description Elue présidente (un rôle essentiellement symbolique en Géorgie), l’ex-diplomate française, candidate du pouvoir, est contestée par l’opposition.\n",
    "----\n",
    "Title A Budapest en Hongrie, un îlot décroissant pour favoriser la transition\n",
    "Image https://img.lemde.fr/2018/12/01/10/0/4214/2809/147/97/60/0/15b32ca_1EY4qISQ_BP4kPAh1fozJdXZ.jpg\n",
    "Description Le centre logistique Cargonomia sert de matrice aux coopératives de l’économie durable et solidaire hongroise.\n",
    "----\n",
    "Title En Israël, la police recommande l’inculpation de Nétanyahou dans une troisième enquête\n",
    "Image https://img.lemde.fr/2018/12/02/167/0/4207/2804/147/97/60/0/9e02c9b_3580d043ebc94b48b0f2cfef4e9a21e7-3580d043ebc94b48b0f2cfef4e9a21e7-0.jpg\n",
    "Description Le premier ministre est soupçonné de corruption, fraude et abus de pouvoir, dans une affaire impliquant le groupe de télécoms israélien Bezeq.\n",
    "----\n",
    "Title Donald Trump veut « mettre fin » à l’Aléna rapidement\n",
    "Image https://img.lemde.fr/2018/11/30/0/0/4861/3240/147/97/60/0/8b87184_5826023-01-06.jpg\n",
    "Description Le président américain souhaite voir disparaître l’accord de libre-échange remontant à 1994 avec le Mexique et le Canada, qu’il qualifie régulièrement de « pire accord jamais signé », en faveur du nouveau traité négocié difficilement avec ses voisins nord-américains ces derniers mois.\n",
    "----\n",
    "Title Le cauchemar de la « rééducation » des musulmans en Chine\n",
    "Image https://img.lemde.fr/2018/11/15/151/0/5000/3333/147/97/60/0/118c78f_248b226e6b91450aa8a68bd0ea5525a8-248b226e6b91450aa8a68bd0ea5525a8-0.jpg\n",
    "Description Ouïgours et Kazakhs du Xinjiang... C’est toute une population musulmane que Pékin veut « rééduquer » en internant des centaines de milliers d’entre eux dans des camps.\n",
    "----\n",
    "Title « AMLO » lance sa transformation du Mexique\n",
    "Image https://img.lemde.fr/2018/12/02/45/0/1497/998/147/97/60/0/a33c174_GGGTBR84_MEXICO-POLITICS-_1202_11.JPG\n",
    "Description Education et santé gratuites, hausse du salaire minimum, bourses scolaires : à peine investi, le président Andres Manuel Lopez Obrador a listé les mesures qu’il entend prendre pour redresser le pays.\n",
    "----\n",
    "Title « Paris brûle » : les médias étrangers relatent le « chaos » en marge des défilés des « gilets jaunes »\n",
    "Image https://img.lemde.fr/2018/12/02/361/0/598/396/147/97/60/0/ba46a6e_XVIt1Ffwm50iYBheccVieUQQ.jpg\n",
    "Description Les images de destructions, d’échauffourées ou de voitures enflammées s’affichaient samedi soir en « une » de nombreux sites d’actualité internationaux.\n",
    "----\n",
    "Title Andrés Manuel Lopez Obrador intronisé président du Mexique\n",
    "Image https://img.lemde.fr/2018/12/02/91/145/1346/897/147/97/60/0/877cd51_a4618baa8da2414bb62bab28a6d4c745-a4618baa8da2414bb62bab28a6d4c745-0.jpg\n",
    "Description Le nouveau chef d’Etat a promis de lutter contre la corruption en menant une transformation « profonde et radicale » du pays.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence des données\n",
    "\n",
    "Pour pouvoir stocker les informations que l'on récupère en parcourant un\n",
    "site il faut pouvoir les stocker. On utilise soit de simples\n",
    "dictionnaires Python, ou mieux des `scrapy.Item` qui sont des\n",
    "dictionnaires améliorés.\n",
    "\n",
    "Nous allons voir les deux façons de faire. On peut réécrire la méthode\n",
    "`parse_category()` pour lui faire retourner un dictionnaire\n",
    "correspondant à chaque offre rencontrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_category(self, response):\n",
    "    for article in response.css(\".fleuve\")[0].css(\"article\"):\n",
    "        title = self.clean_spaces(article.css(\"h3 a::text\").extract_first())\n",
    "        image = article.css(\"img::attr(data-src)\").extract_first()\n",
    "        description = article.css(\".txt3::text\").extract_first()\n",
    "        yield {\n",
    "            \"title\":title,\n",
    "            \"image\":image,\n",
    "            \"description\":description\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on combine tout dans la spider :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy import Request\n",
    "\n",
    "\n",
    "class LemondeSpider(scrapy.Spider):\n",
    "    name = \"lemonde\"\n",
    "    allowed_domains = [\"www.lemonde.fr\"]\n",
    "    start_urls = ['https://www.lemonde.fr']\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').extract_first()\n",
    "        all_links = {\n",
    "            name:response.urljoin(url) for name, url in zip(\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract(),\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::attr(href)\").extract())\n",
    "        }\n",
    "        for link in all_links.values():\n",
    "            yield Request(link, callback=self.parse_category)\n",
    "\n",
    "    def parse_category(self, response):\n",
    "        for article in response.css(\".fleuve\")[0].css(\"article\"):\n",
    "            title = self.clean_spaces(article.css(\"h3 a::text\").extract_first())\n",
    "            image = article.css(\"img::attr(data-src)\").extract_first()\n",
    "            description = article.css(\".txt3::text\").extract_first()\n",
    "            yield {\n",
    "                \"title\":title,\n",
    "                \"image\":image,\n",
    "                \"description\":description\n",
    "            }\n",
    "\n",
    "\n",
    "    def clean_spaces(self, string):\n",
    "        if string:\n",
    "            return \" \".join(string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant lancer notre spider avec la commande suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl <NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">    \n",
    "\n",
    "`scrapy crawl` permet de démarrer le processus en allant chercher la\n",
    "classe `scrapy.Spider` dont l'attribut `name` = &lt;NAME&gt;.\n",
    "\n",
    "Par exemple, pour la spider `LeMondeSpider` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl lemonde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">      \n",
    "> {'title': '« Gilets jaunes » : « La question n’est plus la crise\n",
    "> écologique. Elle est de sortir au plus vite de la violence »',\n",
    "> 'image':\n",
    "> '<https://img.lemde.fr/2018/12/01/0/0/1999/1333/147/97/60/0/bd07906_oYK0XUhof1ma2smWloAu1mbd.jpg>',\n",
    "> 'description': 'L’exécutif n’est pas assuré de pouvoir maintenir la\n",
    "> sécurité et l’ordre en cas de quatrième week-end de mobilisation,\n",
    "> estime l’éditorialiste au «xa0Mondexa0» Françoise Fressoz, au soir\n",
    "> d’une journée d’émeutes dans Paris, samedi.'} 2018-12-02 17:10:03\n",
    "> \\[scrapy.core.scraper\\] DEBUG: Scraped from &lt;200\n",
    "> <https://www.lemonde.fr/mouvement-des-gilets-jaunes/>&gt; {'title': '«\n",
    "> Gilets jaunes » : Emmanuel Macron réagit après les violences à Paris',\n",
    "> 'image':\n",
    "> '<https://img.lemde.fr/2018/12/01/0/4/4920/3280/147/97/60/0/9202b53_91f4a2b01be642658d32730018fbb799-91f4a2b01be642658d32730018fbb799-0.jpg>',\n",
    "> 'description': 'Emmanuel Macron a condamné les violences survenues en\n",
    "> marge des rassemblements des «xa0gilets jaunesxa0», samedi 1er janvier\n",
    "> à Paris.'} 2018-12-02 17:10:03 \\[scrapy.core.scraper\\] DEBUG: Scraped\n",
    "> from &lt;200 <https://www.lemonde.fr/mouvement-des-gilets-jaunes/>&gt;\n",
    "> {'title': '« Gilets jaunes » à Lille : « Ils veulent nous laminer,\n",
    "> mais aujourd’hui, toute notre colère ressort »', 'image':\n",
    "> '<https://img.lemde.fr/2018/12/01/32/0/4323/2879/147/97/60/0/9b3842c_b_34XdIsl-E7T-GBIHr9Vgr8.jpg>',\n",
    "> 'description': 'Près de 2xa0500xa0personnes, selon les organisateurs,\n",
    "> ont manifesté à Lille, sans qu’aucun incident n’ait été signalé.'}\n",
    "> 2018-12-02 17:10:03 \\[scrapy.core.scraper\\] DEBUG: Scraped from\n",
    "> &lt;200 <https://www.lemonde.fr/mouvement-des-gilets-jaunes/>&gt;\n",
    "> {'title': '« Gilets jaunes » : « Les stations-service de certains\n",
    "> territoires ruraux doivent être dispensées de la taxe carbone »',\n",
    "> 'image':\n",
    "> '<https://img.lemde.fr/2018/11/28/118/0/5184/3452/147/97/60/0/84b3ae3_ebokcanGhRHtcS5n75IAdwfr.JPG>',\n",
    "> 'description': 'Dans une tribune au «xa0Mondexa0», l’économiste Alain\n",
    "> Trannoy préconise, en attendant le déploiement d’une véritable\n",
    "> alternative électrique sur tout le territoire, de supprimer la taxe\n",
    "> dans les zones où la mobilité est contrainte.'} 2018-12-02 17:10:03\n",
    "> \\[scrapy.core.scraper\\] DEBUG: Scraped from &lt;200\n",
    "> <https://www.lemonde.fr/mouvement-des-gilets-jaunes/>&gt; {'title':\n",
    "> 'La journée de mobilisation des « gilets jaunes » en images', 'image':\n",
    "> '<https://img.lemde.fr/2018/12/01/0/0/5949/3966/147/97/60/0/fe11fe3_7pHsoBoJhcKJfFDcSyWF5XEV.jpg>',\n",
    "> 'description': 'A Paris et province, des «xa0gilets jaunesxa0» se sont\n",
    "> de nouveau réunis samedi. La journée a été marquée par de graves\n",
    "> violences, notamment dans la capitale.'} 2018-12-02 17:10:03\n",
    "> \\[scrapy.core.scraper\\] DEBUG: Scraped from &lt;200\n",
    "> <https://www.lemonde.fr/mouvement-des-gilets-jaunes/>&gt; {'title': '«\n",
    "> Gilets jaunes » : les images des violences au cœur de Paris', 'image':\n",
    "> '<https://img.lemde.fr/2018/12/01/0/0/4372/2914/147/97/60/0/7e8cfdd_5833490-01-06.jpg>',\n",
    "> 'description': 'Magasins pillés, voitures incendiées, bâtiments\n",
    "> attaqués… les affrontements entre «xa0gilets jaunesxa0» et forces de\n",
    "> l’ordre ont fait plus d’une centaine de blessés.'} 2018-12-02 17:10:03\n",
    "> \\[scrapy.core.scraper\\] DEBUG: Scraped from &lt;200\n",
    "> <https://www.lemonde.fr/mouvement-des-gilets-jaunes/>&gt; {'title':\n",
    "> 'Près de l’Arc de triomphe, les doléances des « gilets jaunes »\n",
    "> recouvertes par le bruit des émeutes', 'image':\n",
    "> '<https://img.lemde.fr/2018/12/01/0/0/1620/1080/147/97/60/0/6eee987_wvZ85RkQO3Q7OVvF7msFvvkS.jpg>',\n",
    "> 'description': 'Toute la journée, les manifestants les plus virulents\n",
    "> et les forces de l’ordre se sont disputé le contrôle des avenues\n",
    "> autour de la place de l’Etoile.'} 2018-12-02 17:10:03\n",
    "> \\[scrapy.core.scraper\\] DEBUG: Scraped from &lt;200\n",
    "> <https://www.lemonde.fr/implant-files/>&gt; {'title': 'Implants\n",
    "> médicaux : « Je suis une miraculée mais j’ai vécu sept ans de calvaire\n",
    "> »', 'image':\n",
    "> '<https://img.lemde.fr/2018/11/22/0/0/2899/1933/147/97/60/0/91a79f9_dwNEK3IQwvsEEpgllltYxA47.jpg>',\n",
    "> 'description': 'Douleurs insupportables, manque de réaction et\n",
    "> d’information de la part de médecins, explantation difficile… Trois\n",
    "> femmes témoignent des conséquences de la pose d’un dispositif médical\n",
    "> dans leur corps.'}\n",
    "\n",
    "On peut exporter les résultats de ces retours dans différents formats de\n",
    "fichiers.\n",
    "\n",
    "-   CSV : $scrapy crawl lemonde -o lbc.csv$\n",
    "-   JSON : $scrapy crawl lemonde -o lbc.json$\n",
    "-   JSONLINE : $scrapy crawl lemonde -o lbc.jl$\n",
    "-   XML : $scrapy crawl lemonde -o lbc.xml$\n",
    "\n",
    "Note\n",
    "\n",
    "Exercice : Exécuter la spider avec les différents formats de stockage.\n",
    "Explorer ensuite le contenu des fichiers ainsi créés.\n",
    "\n",
    "## Votre premier Item\n",
    "\n",
    "La classe `Item` permet de structurer les données que l'on souhaite\n",
    "récupérer sous la forme d'un modèle. Les items doivent être définis dans\n",
    "le fichier `items.py` créé par la commande `scrapy startproject`. Les\n",
    "`Item` héritent de la class `scrapy.Item`.\n",
    "\n",
    "On veut structurer les données avec deux champs : le titre et le prix de\n",
    "l'annonce. Scrapy utilise une classe `scrapy.Field` permettant de\n",
    "'déclarer' ces champs. Dans notre cas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ArticleItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    image = scrapy.Field()\n",
    "    description = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser la classe `scrapy.Item` plutôt qu'un simple dictionnaire permet\n",
    "plus de contrôle sur la structure des données. En effet, on ne peut\n",
    "insérer dans les items que des données avec des clés 'déclarées'. Ce qui\n",
    "assure une plus grande cohérence au sein d'un projet.\n",
    "\n",
    "On peut instancier un item de plusieurs façons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [4]: article_item = ArticleItem(title=\"Gilets Jaunes\", image=None, description=\"Un samedi de manifestations\")\n",
    "\n",
    "In [5]: print(article_item)\n",
    "{'description': 'Un samedi de manifestations',\n",
    "'image': None,\n",
    "'title': 'Gilets Jaunes'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [9]: article_item = ArticleItem()\n",
    "    ...: article_item[\"title\"] = 'Gilets Jaunes'\n",
    "    ...: article_item[\"description\"] = 'Un samedi de manifestations'\n",
    "    ...:\n",
    "\n",
    "In [10]: print(article_item)\n",
    "{'description': 'Un samedi de manifestations', 'title': 'Gilets Jaunes'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La définition d'un item permet de palier toutes les erreurs de typo dans\n",
    "les champs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [11]: article_item = ArticleItem()\n",
    "    ...: article_item[\"titel\"] = 'Gilets Jaunes'\n",
    "    ...:\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "<ipython-input-11-de371261a7a5> in <module>()\n",
    "    1 article_item = ArticleItem()\n",
    "----> 2 article_item[\"titel\"] = 'Gilets Jaunes'\n",
    "\n",
    "~/anaconda3/lib/python3.6/site-packages/Scrapy-1.3.3-py3.6.egg/scrapy/item.py in __setitem__(self, key, value)\n",
    "    64         else:\n",
    "    65             raise KeyError(\"%s does not support field: %s\" %\n",
    "---> 66                 (self.__class__.__name__, key))\n",
    "    67\n",
    "    68     def __delitem__(self, key):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KeyError: 'ArticleItem does not support field: titel'\n",
    "\n",
    "Les items héritent des dictionnaires Python, et possèdent donc toutes\n",
    "les méthodes de ceux-ci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [13]: article_item = ArticleItem(title=\"Gilets Jaunes\")\n",
    "    ...: print(article_item[\"title\"]) # Méthode __getitem__()\n",
    "    ...: print(article_item.get(\"description\", \"no description\")) # Méthode get()\n",
    "    ...:\n",
    "Gilets Jaunes\n",
    "no description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut transformer un `Item` en dictionnaire très facilement, en le\n",
    "passant au constructeur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_item = ArticleItem(title=\"Drone DJI\")\n",
    "print(type(article_item))\n",
    "dict_item = dict(article_item)\n",
    "print(type(dict_item))\n",
    "print(dict_item)\n",
    "\n",
    "<class '__main__.ArticleItem'>\n",
    "<class 'dict'>\n",
    "{'title': 'Drone DJI'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On intègre maintenant cet item dans notre spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from scrapy import Request\n",
    "from ..items import ArticleItem\n",
    "class LemondeSpider(scrapy.Spider):\n",
    "    name = \"lemonde\"\n",
    "    allowed_domains = [\"www.lemonde.fr\"]\n",
    "    start_urls = ['https://www.lemonde.fr']\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').extract_first()\n",
    "        all_links = {\n",
    "            name:response.urljoin(url) for name, url in zip(\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract(),\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::attr(href)\").extract())\n",
    "        }\n",
    "        for link in all_links.values():\n",
    "            yield Request(link, callback=self.parse_category)\n",
    "\n",
    "    def parse_category(self, response):\n",
    "        for article in response.css(\".fleuve\")[0].css(\"article\"):\n",
    "            title = self.clean_spaces(article.css(\"h3 a::text\").extract_first())\n",
    "            image = article.css(\"img::attr(data-src)\").extract_first()\n",
    "            description = article.css(\".txt3::text\").extract_first()\n",
    "            #yield {\n",
    "            #    \"title\":title,\n",
    "            #    \"image\":image,\n",
    "            #    \"description\":description\n",
    "            #}\n",
    "\n",
    "            yield ArticleItem(\n",
    "                title=title,\n",
    "                image=image,\n",
    "                description=description\n",
    "            )\n",
    "\n",
    "    def clean_spaces(self, string):\n",
    "        if string:\n",
    "            return \" \".join(string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On voit bien que le générateur retourne maintenant un `Item`.\n",
    ">\n",
    "> Note\n",
    ">\n",
    "> Exercice :\n",
    ">\n",
    "> Relancer la spider pour vérifier le bon déroulement de l'extraction.\n",
    "\n",
    "## Postprocessing\n",
    "\n",
    "Si l'on se réfère au diagramme d'architecture de Scrapy, on voit qu'il\n",
    "est possible d'insérer des composants supplémentaires dans le flux de\n",
    "traitement. Ces composants s'appellent `Pipelines`.\n",
    "\n",
    "Par défaut, tous les `Item` générés au sein d'un projet Scrapy passent\n",
    "par les `Pipelines`. Les pipelines sont utilisés la plupart du temps\n",
    "pour :\n",
    "\n",
    "-   Nettoyer du contenu HTML ;\n",
    "-   Valider les données scrapées ;\n",
    "-   Supprimer les items qu'on ne souhaite pas stocker ;\n",
    "-   Stocker ces objets dans des bases de données.\n",
    "\n",
    "Les pipelines doivent être définis dans le fichier `pipelines.py`.\n",
    "\n",
    "Dans notre cas on peut vouloir nettoyer le champ `title` pour enlever\n",
    "les caractères superflus.\n",
    "\n",
    "Nous allons alors transferer la fonction de nettoyage du code html dans\n",
    "une Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class TextPipeline(object):\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if item['title']:\n",
    "            item[\"title\"] = clean_spaces(item[\"title\"])\n",
    "            return item\n",
    "        else:\n",
    "            raise DropItem(\"Missing title in %s\" % item)\n",
    "\n",
    "\n",
    "def clean_spaces(string):\n",
    "    if string:\n",
    "        return \" \".join(string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour dire au process Scrapy de faire transiter les items par ces\n",
    "pipelines. Il faut le spécifier dans le fichier de paramétrage\n",
    "`settings.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "     'newscrawler.pipelines.TextPipeline': 300,\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant supprimer la fonction `clean_spaces()` de\n",
    "l'extraction des données et laisser le Pipeline faire son travail. La\n",
    "valeur entière définie permet de déterminer l'ordre dans lequel les\n",
    "pipelines vont être appelés. Ces entiers peuvent être compris entre 0 et\n",
    "1000.\n",
    "\n",
    "On relance notre spider :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl lemonde -o ../data/articles.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi utiliser les Pipelines pour stocker les données récupérées\n",
    "dans une base de données. Pour stocker les items dans des documents\n",
    "mongo :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "class MongoPipeline(object):\n",
    "\n",
    "    collection_name = 'scrapy_items'\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = pymongo.MongoClient()\n",
    "        self.db = self.client[\"lemonde\"]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.db[self.collection_name].insert_one(dict(item))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on redéfinit deux autres méthodes: `open_spider()`et\n",
    "`close_spider()`, ces méthodes sont appelées comme leur nom l'indique,\n",
    "lorsque la Spider est instanciée et fermée.\n",
    "\n",
    "Ces méthodes nous permettent d'ouvrir la connexion Mongo et de la fermer\n",
    "lorsque le scraping se termine. La méthode `process_item()` est appelé à\n",
    "chaque fois qu'un item passe dans le mécanisme interne de scrapy. Ici,\n",
    "la méthode permet d'insérer l'item en tant que document mongo.\n",
    "\n",
    "Pour que ce pipeline soit appelé il faut l'ajouter dans les settings du\n",
    "projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "    'newscrawler.pipelines.TextPipeline': 100,\n",
    "    'newscrawler.pipelines.MongoPipeline': 300\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pipeline est ajoutée à la fin du process pour profiter des deux\n",
    "précédents.\n",
    "\n",
    "## Settings\n",
    "\n",
    "Scrapy permet de gérer le comportement des spiders avec certains\n",
    "paramètres. Comme expliqué dans le premier cours, il est important de\n",
    "suivre des règles en respectant la structure des différents sites. Il\n",
    "existe énormément de paramètres mais nous allons (dans le cadre de ce\n",
    "cours) aborder les plus utilisés :\n",
    "\n",
    "-   DOWNLOAD\\_DELAY : Le temps de téléchargement entre chaque requête\n",
    "    sur le même domaine ;\n",
    "-   CONCURRENT\\_REQUESTS\\_PER\\_DOMAIN : Nombre de requêtes simultanées\n",
    "    par domaine ;\n",
    "-   CONCURRENT\\_REQUESTS\\_PER\\_IP : Nombre de requêtes simultanées par\n",
    "    IP ;\n",
    "-   DEFAULT\\_REQUEST\\_HEADERS : Headers HTTP utilisés pour les requêtes\n",
    "    ;\n",
    "-   ROBOTSTXT\\_OBEY : Scrapy récupère le robots.txt et adapte le\n",
    "    scraping en fonction des règles trouvées ;\n",
    "-   USER\\_AGENT : UserAgent utilisé pour faire les requêtes ;\n",
    "-   BOT\\_NAME : Nom du bot annoncé lors des requêtes\n",
    "-   HTTPCACHE\\_ENABLED : Utilisation du cache HTTP, utile lors du\n",
    "    parcours multiple de la même page.\n",
    "\n",
    "Le fichiers `settings.py` permet de définir les paramètres globaux d'un\n",
    "projet. Si votre projet contient un grand nombre de spiders, il peut\n",
    "être intéressant d'avoir des paramètres distincts pour chaque spider. Un\n",
    "moyen simple est d'ajouter un attribut `custom_settings` à votre spider\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeMondeSpider(scrapy.Spider):\n",
    "        name = \"lemonde\"\n",
    "        allowed_domains = [\"lemonde.fr\"]\n",
    "        start_urls = ['http://lemonde.fr/']\n",
    "        custom_settings = {\n",
    "            \"HTTPCACHE_ENABLED\":True, \n",
    "            \"CONCURRENT_REQUESTS_PER_DOMAIN\":100\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
